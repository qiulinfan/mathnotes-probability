\chapter*{Homework 3}
\section*{Problem 1}
Let $Z$ be a standard normal random variable $Z \sim N(0,1)$. We denote by $\Phi$ its distribution function. Answer the questions below
\begin{itemize}
    \item [(a)] If $a, b \in \mathbb{R}$ with $a>0$, show that the random variable $a Z+b$ 
    is also normal and find its mean and variance.
    \item [(b)] Show that $\Phi(0)=1 / 2$.
    \item [(c)] Show that $\Phi(-x)=1-\Phi(x)$ for any $x \in \mathbb{R}$.
\end{itemize}
\begin{solution}
\begin{itemize}
\item [(a)]
$Z\sim N(0,1)$ has density
\[
f_Z(z)=\frac{1}{\sqrt{2\pi}}e^{-z^2/2}
\]

\begin{align*}
    F_X(x) =\bP(X \leq x) &= \bP(aZ+b \leq x) \\
    &= \bP\left(Z \leq \frac{x-b}{a}\right) \\
    & = \Phi\left(\frac{x-b}{a}\right)
\end{align*}
Thus \[
f_X(x) = \frac{d}{dx} \Phi\left(\frac{x-b}{a}\right) =
 \frac{1}{a} \varphi\left(\frac{x-b}{a}\right) = 
 \frac{1}{\sqrt{a^22\pi}} e^{-\frac{(x-b)^2}{2a^2}}
\]

Note this is the density of a normal distribution with mean $b$ and variance $a^2$. 
Therefore
\[
aZ+b \sim N(b,a^2)
\]
Since $Z$ has mean $0$ and variance $1$, use linearity we have
\[
\mathbb E[aZ+b]=a\mathbb E[Z]+b=b
\]
and 
\[
\mathrm{Var}(aZ+b)=a^2\mathrm{Var}(Z)=a^2
\]
\item [(b)] 
Note the standard normal density is an even function:
\[
\varphi(x)=\frac{1}{\sqrt{2\pi}}e^{-x^2/2} = \varphi(-x)
\]
Thus
\[
\Phi(0)=\int_{-\infty}^{0}\varphi(x)\,dx
=\int_{0}^{\infty}\varphi(x)\,dx
\]
Since $\int_{-\infty}^{\infty}\varphi(x)\,dx=1$, 
the two equal halves are each $1/2$, so $\Phi(0)=1/2$.
\item [(c)]
For any $x\in\mathbb R$,
\[
\Phi(-x)=\int_{-\infty}^{-x}\varphi(t)\,dt
\]
Let $u=-t$, using $\varphi(-u)=\varphi(u)$ we have
\[
\Phi(-x)=\int_{\infty}^{x}\varphi(-u)(-du)=\int_{x}^{\infty}\varphi(u)\,du
=1-\int_{-\infty}^{x}\varphi(u)\,du
=1-\Phi(x)
\]
\end{itemize}

\end{solution}





\section*{Problem 2}
Let $X$ and $Y$ be random variables with joint density
$$
f(x, y)= \begin{cases}-x y, & (x, y) \in(-1,0) \times(0,1) \cup(1,2) \times(-1,0), \\ 0, 
& \text { otherwise }\end{cases}
$$
\begin{itemize}
    \item [(a)] Compute the probability $\mathbb{P}(X+Y<0)$.
    \item [(b)] Compute the expected value $\mathbb{E}[X Y]$.
    \item [(c)] Are $X$ and $Y$ independent?
\end{itemize}
\begin{solution}
\begin{itemize}
\item [(a)]
On $(1,2)\times(-1,0)$ we have $x+y>0$ since $x>1$ and $y>-1$, 
hence this region contributes nothing to $\{X+Y<0\}$.

On $(-1,0)\times(0,1)$, the ineq $x+y<0$ is equivalent to $0<y<-x$. Therefore,
\[
\mathbb P(X+Y<0)=\int_{-1}^{0}\int_{0}^{-x} (-xy)\,dy\,dx
\]
Compute the inner integral:
\[
\int_{0}^{-x} (-xy)\,dy=-x\cdot \frac{(-x)^2}{2}=-\frac{x^3}{2}
\]
Hence,
\[
\mathbb P(X+Y<0)=\int_{-1}^{0}\left(-\frac{x^3}{2}\right)dx
=-\frac12\cdot \frac{x^4}{4}\Big|_{-1}^{0}=\frac18
\]

\item [(b)]
By def,
\begin{align*}
\mathbb E[XY]&=\int_{\bR^2} xy\,f(x,y)\,dx\,dy \\
&=-\int_{(-1,0)\times(0,1)\cup(1,2)\times(-1,0)} x^2y^2\,dx\,dy     \\
&= -\int_{-1}^{0}\int_{0}^{1} x^2y^2\,dy\,dx  - \int_{1}^{2}\int_{-1}^{0} x^2y^2\,dy\,dx
\end{align*}

Split over the two rectangles.
On $(-1,0)\times(0,1)$,
\[
-\int_{-1}^{0}\int_{0}^{1} x^2y^2\,dy\,dx
=-\left(\int_{-1}^{0}x^2\,dx\right)\left(\int_{0}^{1}y^2\,dy\right)
=-\left(\frac13\right)\left(\frac13\right)=-\frac19
\]
On $(1,2)\times(-1,0)$,
\[
-\int_{1}^{2}\int_{-1}^{0} x^2y^2\,dy\,dx
=-\left(\int_{1}^{2}x^2\,dx\right)\left(\int_{-1}^{0}y^2\,dy\right)
=-\left(\frac73\right)\left(\frac13\right)=-\frac79
\]
Thus,
\[
\mathbb E[XY]=-\frac19-\frac79=-\frac89
\]

\item [(c)]
Consider: For $x\in(-1,0)$,
\[
f_X(x)=\int_{0}^{1}(-xy)\,dy=\frac{-x}{2}
\]
For $y\in(0,1)$,
\[
f_Y(y)=\int_{-1}^{0}(-xy)\,dx=y\int_{-1}^{0}(-x)\,dx=\frac{y}{2}
\]
And for $(x,y)\in(-1,0)\times(0,1)$,
\[
f_X(x)f_Y(y)=\left(\frac{-x}{2}\right)\left(\frac{y}{2}\right)=\frac{-xy}{4}\neq -xy=f(x,y)
\]
Thus $X$ and $Y$ are not independent.
\end{itemize}

\end{solution}


\section*{Problem 3}
Let $X \sim \operatorname{Exp}(1)$ and $Y=X+\frac{1}{X+1}$. 
Find $\mathbb{P}((X+1) Y \leq 2)$ and $\operatorname{Cov}(X, Y)$.\\
Hint: You may leave your answer as a function of the integral $\int_0^{\infty} \frac{e^{-x}}{1+x} d x$.
\begin{solution}
Note 
\[
(X+1)Y=(X+1)\left(X+\frac{1}{X+1}\right)=X(X+1)+1=X^2+X+1
\]
Thus,
\[
(X+1)Y\le 2 \iff X^2+X-1\le 0
\]
The roots of $x^2+x-1=0$ are $\frac{-1\pm \sqrt5}{2}$. Since $X\ge 0$, the event is
\[
0\le X\le \frac{\sqrt5-1}{2}
\]
Therefore, using the CDF of $\mathrm{Exp}(1)$,
\[
\mathbb P((X+1)Y\le 2)=\mathbb P(X\le \frac{\sqrt5-1}{2})=1-e^{-\frac{\sqrt5-1}{2}}
=1-\exp\!\left(-\frac{\sqrt5-1}{2}\right)
\]
Nowe we compute the covariance. By def,
\[
\mathrm{Cov}(X,Y)=\mathbb E[XY]-\mathbb E[X]\mathbb E[Y]
\]
For $X\sim\mathrm{Exp}(1)$, $\mathbb E[X]=1$ and $\mathbb E[X^2]=2$. Let
\[
I:=\int_{0}^{\infty}\frac{e^{-x}}{1+x}\,dx=\mathbb E\!\left[\frac{1}{1+X}\right]
\]
Then
\[
\mathbb E[Y]=\mathbb E[X]+\mathbb E\!\left[\frac{1}{1+X}\right]=1+I
\]
Also,
\[
XY=X\left(X+\frac{1}{1+X}\right)=X^2+\frac{X}{1+X}
=X^2+\left(1-\frac{1}{1+X}\right)
\]
so
\[
\mathbb E[XY]=\mathbb E[X^2]+1-\mathbb E\!\left[\frac{1}{1+X}\right]=2+1-I=3-I
\]
Hence,
\[
\mathrm{Cov}(X,Y)=(3-I)-(1)(1+I)=2-2I
=2-2\int_{0}^{\infty}\frac{e^{-x}}{1+x}\,dx
\]
\end{solution}


\section*{Problem 4}
Find the conditional density $f_{Y \mid X}(y \mid x)$ of $Y$
 given that $X=x$ and the corresponding conditional expectation 
 $\mathbb{E}[Y \mid X=x]$ if the pair of random variables $(X, Y)$ 
 has absolutely continuous distribution with joint density: 
 $f_{X, Y}(x, y)=\lambda^2 e^{-\lambda y} \mathbf{1}_{\{0 \leq x \leq y\}}$.
\begin{solution}
Given the joint density
\(
f_{X,Y}(x,y)=\lambda^2 e^{-\lambda y}\mathbf 1_{\{0\le x\le y\}}
\), we first compute the marginal density of $X$. For $x\ge 0$,
\[
f_X(x)=\int_{y=x}^{\infty}\lambda^2 e^{-\lambda y}\,dy
=\lambda^2\cdot \frac{e^{-\lambda x}}{\lambda}
=\lambda e^{-\lambda x}
\]
and $f_X(x)=0$ for $x<0$.

Therefore, for $x\ge 0$,
\[
f_{Y|X}(y|x)=\frac{f_{X,Y}(x,y)}{f_X(x)}
=\frac{\lambda^2 e^{-\lambda y}\mathbf 1_{\{y\ge x\}}}{\lambda e^{-\lambda x}}
=\lambda e^{-\lambda (y-x)}\mathbf 1_{\{y\ge x\}}
\]
This shows that $Y|X=x$ has the same distribution as $x+E$ where $E\sim \mathrm{Exp}(\lambda)$, hence
\[
\mathbb E[Y|X=x]=x+\frac{1}{\lambda}
\]
\end{solution}



\section*{Problem 5}
A machine produces a coin that shows heads with a random probability $p$. 
The value of $p$ is unknown to us, but from many observations of the coins produced by the machine 
we know that the distribution of the random parameter $p$ is uniform on $(0,1 / 2)$. 
We start tossing the coin. Compute the following probabilities:
\begin{itemize}
    \item [(a)] The coin shows heads on the first toss.
    \item [(b)] The expected number of tosses until tails show up.
\end{itemize}
\begin{solution}

\begin{itemize}
\item [(a)] 
The head probability $p\sim \mathrm{Unif}(0,1/2)$. Thus the density is:
\[
f_P(p)=2\,\mathbf 1_{(0,1/2)}(p)
\]
The unconditional probability of heads on the first toss is
\[
\mathbb P(\text{H on first toss})=\mathbb E[p]
=\int_0^{1/2} p\cdot 2\,dp
=2\cdot \frac{p^2}{2}\Big|_{0}^{1/2}
=\frac14
\]

\item [(b)]
Let $T$ be the number of tosses until the first tail occurs. 
Conditional on $p$, tails occurs with probability $1-p$ each toss, 
so $T$ is geometric with parameter $1-p$.
Hence
\[
\mathbb E[T\,|\,p]=\frac{1}{1-p}
\]
Taking expectation over $p$,
\[
\mathbb E[T]=\mathbb E\!\left[\frac{1}{1-p}\right]
=\int_0^{1/2}\frac{1}{1-p}\cdot 2\,dp
=2\big[-\ln(1-p)\big]_{0}^{1/2}
=2\ln 2
\]
\end{itemize}


\end{solution}


\section*{Problem 6}
The joint probability density function of the random variables $X$ and $Y$ is given by
$$
f_{X, Y}(x, y)= \begin{cases}c\left(x^2+\frac{x y}{2}\right), & (x, y) \in(0,1) \times(0,2) \\ 
0, & \text { otherwise }\end{cases}
$$
\begin{itemize}
    \item [(a)] Find the constant $c$.
    \item [(b)] Find the marginal density of $X$ and compute $\mathbb{E}[X]$.
    \item [(c)] Compute $\mathbb{P}(X>Y)$.
    \item [(d)] Compute $\mathbb{P}\left(\left.Y>\frac{1}{2} \right\rvert\, X<\frac{1}{2}\right)$.  
\end{itemize}
\begin{solution}
\begin{itemize}
\item [(a)]
Determine $c$ from normalization:
\[
1=\int_0^1\int_0^2 c\left(x^2+\frac{xy}{2}\right)\,dy\,dx
\]
For fixed $x$,
\[
\int_0^2\left(x^2+\frac{xy}{2}\right)dy
=2x^2+\frac{x}{2}\cdot\frac{y^2}{2}\Big|_{0}^{2}
=2x^2+x
\]
Thus
\[
1=c\int_0^1(2x^2+x)\,dx
=c\left(\frac{2}{3}+\frac12\right)
=c\cdot\frac{7}{6}
\]
so $c=\frac{6}{7}$

\item [(b)]
The marginal density of $X$ (for $0<x<1$) is
\[
f_X(x)=\int_0^2 c\left(x^2+\frac{xy}{2}\right)\,dy=c(2x^2+x)=\frac{6}{7}(2x^2+x)
\]
and $f_X(x)=0$ otherwise.

Thus
\[
\mathbb E[X]=\int_0^1 x f_X(x)\,dx
=\frac{6}{7}\int_0^1(2x^3+x^2)\,dx
=\frac{6}{7}\left(\frac12+\frac13\right)=\frac{5}{7}
\]

\item [(c)]
The event $\{X>Y\}$ corresponds to the region $0<y<x<1$ (since $x\in(0,1)$). Hence
\[
\mathbb P(X>Y)=\int_0^1\int_0^x c\left(x^2+\frac{xy}{2}\right)\,dy\,dx
\]
For fixed $x$,
\[
\int_0^x\left(x^2+\frac{xy}{2}\right)dy
=x^3+\frac{x}{2}\cdot\frac{y^2}{2}\Big|_{0}^{x}
=x^3+\frac{x^3}{4}=\frac{5}{4}x^3
\]
Therefore
\[
\mathbb P(X>Y)=c\int_0^1\frac{5}{4}x^3\,dx
=c\cdot\frac{5}{4}\cdot\frac14
=c\cdot\frac{5}{16}
=\frac{6}{7}\cdot\frac{5}{16}
=\frac{15}{56}
\]

\item [(d)]
By definition,
\[
\mathbb P\!\left(\left.Y>\frac12\right|X<\frac12\right)
=\frac{\mathbb P\left(Y>\frac12,\ X<\frac12\right)}{\mathbb P\left(X<\frac12\right)}
\]
Calculate each part. First the denominator:
\[
\mathbb P\left(X<\frac12\right)=\int_0^{1/2} f_X(x)\,dx
=c\int_0^{1/2}(2x^2+x)\,dx
=c\left(\frac{1}{12}+\frac18\right)
=c\cdot\frac{5}{24}
=\frac{5}{28}
\]
And the numerator:
\[
\mathbb P\left(Y>\frac12,\ X<\frac12\right)
=\int_0^{1/2}\int_{1/2}^{2} c\left(x^2+\frac{xy}{2}\right)\,dy\,dx
\]
For fixed $x$,
\[
\int_{1/2}^{2}\left(x^2+\frac{xy}{2}\right)dy
=x^2\left(2-\frac12\right)+\frac{x}{2}\cdot\frac{y^2}{2}\Big|_{1/2}^{2}
=\frac{3}{2}x^2+\frac{x}{4}\left(4-\frac14\right)
=\frac{3}{2}x^2+\frac{15}{16}x
\]
Thus
\[
\mathbb P\left(Y>\frac12,\ X<\frac12\right)
=c\int_0^{1/2}\left(\frac{3}{2}x^2+\frac{15}{16}x\right)dx
=c\left(\frac{1}{16}+\frac{15}{128}\right)
=c\cdot\frac{23}{128}
=\frac{69}{448}
\]
Therefore,
\[
\mathbb P\!\left(\left.Y>\frac12\right|X<\frac12\right)
=\frac{69/448}{5/28}
=\frac{69}{80}
\]
\end{itemize}
\end{solution}